{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "name": "Rapport-TP3-ForwardBackward_Deveaux_Cheikh_Phan.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSUOFWvXwSlU"
      },
      "source": [
        "# TP Forward Backward and FISTA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z39iUDZpwSlX"
      },
      "source": [
        "# Forward Backward and FISTA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdzC4S_8wSlY"
      },
      "source": [
        "In this notebook we will code the Forward-Backward aLgorithm and its inertial an accelerated version FISTA to solve several optimization problems associated for example to the denoising and the inpaiting problem. I provide two images but you can use one of your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmt-nXcuwSlY"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as scp\n",
        "import pylab as pyl\n",
        "import pywt\n",
        "import pandas as pd\n",
        "import holoviews as hv\n",
        "import param\n",
        "import panel as pn\n",
        "import requests\n",
        "import numpy.linalg as npl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from panel.pane import LaTeX\n",
        "hv.extension('bokeh')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from PIL import Image\n",
        "from io import BytesIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD3fvZCewSlZ"
      },
      "source": [
        "local=0\n",
        "def chargeData(name):\n",
        "    if local:\n",
        "        if name=='Lenna':\n",
        "            res=np.array(Image.open(\"./Archive/img/Lenna.jpg\")).astype(float)\n",
        "        if name=='Canaletto':\n",
        "            res=np.array(Image.open(\"./Archive/img/Canaletto.jpeg\")).astype(float)\n",
        "        if name=='Minotaure':\n",
        "            res=np.array(Image.open(\"./Archive/img/MinotaureBruite.jpeg\")).astype(float)   \n",
        "        if name=='Cartoon':\n",
        "            res=np.array(Image.open(\"./Archive/img/Cartoon.jpg\")).astype(float) \n",
        "    else:\n",
        "        if name=='Lenna':\n",
        "            url='https://plmlab.math.cnrs.fr/dossal/optimisationpourlimage/raw/master/img/Lenna.jpg'        \n",
        "            response = requests.get(url)\n",
        "            res=np.array(Image.open(BytesIO(response.content))).astype(float)\n",
        "        if name=='Canaletto':\n",
        "            url='https://plmlab.math.cnrs.fr/dossal/optimisationpourlimage/raw/master/img/Canaletto.jpeg'\n",
        "            response = requests.get(url)\n",
        "            res=np.array(Image.open(BytesIO(response.content))).astype(float)\n",
        "        if name=='Minotaure':\n",
        "            url='https://plmlab.math.cnrs.fr/dossal/optimisationpourlimage/raw/master/img/MinotaureBruite.jpeg'\n",
        "            response = requests.get(url)\n",
        "            res=np.array(Image.open(BytesIO(response.content))).astype(float)\n",
        "        if name=='Cartoon':\n",
        "            url='https://plmlab.math.cnrs.fr/dossal/optimisationpourlimage/raw/master/img/Cartoon.jpg'        \n",
        "            response = requests.get(url)\n",
        "            res=np.array(Image.open(BytesIO(response.content))).astype(float)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxmXSiW9wSla"
      },
      "source": [
        "im=chargeData('Canaletto')\n",
        "im2=chargeData('Lenna')\n",
        "imagesRef= {\"Lenna\" : im2,\"Canaletto\" : im}\n",
        "options = dict(cmap='gray',xaxis=None,yaxis=None,width=400,height=400,toolbar=None)\n",
        "pn.Row(hv.Image(im).opts(**options),hv.Image(im2).opts(**options))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IMFLJeowSla"
      },
      "source": [
        "def PSNR(I,Iref):\n",
        "    temp=I.ravel()\n",
        "    tempref=Iref.ravel()\n",
        "    NbP=I.size\n",
        "    EQM=np.sum((temp-tempref)**2)/NbP\n",
        "    b=np.max(np.abs(tempref))**2\n",
        "    return 10*np.log10(b/EQM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CrQhoFrwSlb"
      },
      "source": [
        "# Inpainting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSTuV2kmwSlb"
      },
      "source": [
        "For the inpainting problem, we suppose we have data $y=Mx^0+b$ which are images of a target image $x^0$ through a Masking operator $M$ and we will consider also that data may be corrupted by an additional noise $b$. We want to estimate $x^0$ solving the following optimization problem \n",
        "\\begin{equation}\\label{Eq1}\n",
        "\\frac{1}{2}\\Vert Mx-y\\Vert^2+\\lambda \\Vert Tx\\Vert_1\n",
        "\\end{equation}\n",
        "where $T$ is an orthogonal transformation such that $Tx^0$ is sparse. If $x^0$ is piecewise smooth, we can use an orthogonal wavelet transform such as Daubechies 2 or 4. To perform the FB algorithm, we need to compute the gradient of $\\frac{1}{2}\\Vert Mx-y\\Vert^2$ and the proximal operator $\\lambda \\Vert Tx\\Vert_1$. Some help is provioded further in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSrpa6JHwSlc"
      },
      "source": [
        "If the noise $b$ vanishes, i.e $y=Mx^0$ where $M$ is the masking operator, it may be better to solve the following optimization problem :\n",
        "\\begin{equation}\\label{Eq2}\n",
        "\\underset{x}{\\min}\\Vert Tx\\Vert_1\\text{ under the constraint }y=Mx  \n",
        "\\end{equation}\n",
        "This problem can't be solved directly using the FB algorithm since it can't be stated as the sum of a differentiable function which gradient is Lipschitz and of a function which proximal operator is known.\n",
        "\n",
        "Moreover, in any pratical problem, there is actually some noise, even it can be small.\n",
        "\n",
        "It can be shown that solutions of \\eqref{Eq1} converge to solutions of \\eqref{Eq2} when $\\lambda$ tends to 0. \n",
        "The advantage of \\eqref{Eq1} is that it can be solved using FB.\n",
        "Solving \\eqref{Eq1} allows to inpaint and to regularize (somehow remove some noise). The regularization effect grows with $\\lambda$. In many practical case, with small noise, $\\lambda$ must be chosen quiete small in \\eqref{Eq1}. \n",
        "The main drawback of such a choice is that it slows down the algorithm, an higher number of iterations is needed to get a good numerical result. When we use these methods, we must be aware that $$\\textbf{the choice of $\\lambda$ of the number of iterations and the starting point of this algorithm is essential}$$. If the numerical resultats are not as nice as you wish, you may have to change the value of $\\lambda$ or increase the number of iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgqCL5sXwSlc"
      },
      "source": [
        "# A few known results on explicit Gradient Descent and Forward-backward (FB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6mBDCZtwSld"
      },
      "source": [
        "If $F=f+g$ is a convex fucntion, sum of two convex functions, $f$ a differentiable function which gradient is $L-$Lipschitz and $g$ a convex function such that $prox_g$ is known, the Forward-Backward algorithm is defined by \n",
        "$$x_{n+1}=prox_{hg}(x_n-h\\nabla f(x_n))=Tx_n\\quad \\text{ with }T:=prox_{hg}\\circ (Id-h\\nabla f)$$\n",
        "One can show that the sequence $(F(x_n)-F(x^*))_{n\\in\\mathbb{N}}$ is non increasing and moreover \n",
        "$$F(x_n)-F(x^*)\\leqslant \\frac{2\\Vert x_0-x^*\\Vert^2}{hn}$$\n",
        "This decay rate $\\frac{1}{n}$ is optimal in the sens that it is impossible to get a bound decaying like $\\frac{1}{n^{\\delta}}$ with $\\delta>1$ for all convex fucntions. Nevertheless it can be proved that if $h<\\frac{1}{L}$ one actually has\n",
        "$$F(x_n)-F(x^*)=o\\left(\\frac{1}{n}\\right).$$  \n",
        "If the function $f$ is strongly convex the decay is geometrical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GZwcrbpwSld"
      },
      "source": [
        "# Some rermarks on iteratives algorithms and FB in particular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmXGO_hXwSle"
      },
      "source": [
        "When you use an iterative algorithm depending on one or more parameters, you must be careful to evaluate the results, the outputs of the algorithm. It is crucial to check some points :\n",
        "\n",
        "1) Check that the algorithm is close to converge... that is check that enough iterations have been done. This number of iterations may change depending on parameters and on the starting point $x_0$ chosen to build ythe sequence.\n",
        "\n",
        "To check this point, I Invite you to display the curve of the values $F(x_n)$ and to think twice about what you doing and what you see on the outputs. For example, if you are performing inpainting and if some black pixels left, or if the sequence $F(x_n)$ is still decaying quite fast, it means that it may be relevant to increse the number of iterations. More generally, observing the artefacts of defects on the output may give some clues of what can be done to improve the result. Moreover, have a look to the sequence $F(x_n)$ may indicate that less iterations could have been performed to get a similar output.\n",
        "\n",
        "2) Explore the possible values of paramters. It may occur that some values of parameters provide some relevant results and some don't. Before pretending that an algorithm is not efficient you must ensure that the parameters have been chosen wisely. These parameters can be the ones of the fucntion to minimize such as $\\lambda$ in \\eqref{Eq1} or the internal parameters of the algorithm such as the descent step for example. Making an Experiments Plan may be a good way to estimate the values of good parameters.\n",
        "\n",
        "One can observe that the value of $F(x)$ is not the only criterion to evaluate the output of an algorithm, actually for different values of $\\lambda$, the PSNR may be more relevant. We can't avoid thinking... \n",
        "\n",
        "3) When it it is possible choose a good starting point. Iterative algorithms build images (or signals) which converges to a minimiozer of the fucntion $F$. the choice of the staring point may be crucial. \n",
        "If we do not have any information, we can choose the constant image 0 or any relevant image, for example the masked image if we are performing inapainting or the noisy image if we are performing denoising. If for any problem a simple and quick method may provide a rough or innacurate output, it can be used as a starting point of the method. For the inpainting, we may start using a simple median filtering. The image is divided into small squares and the holes are filled up with the median value of the observed data.\n",
        "\n",
        "One can often distinguish two steps during the optimization of the function $F$, a first one to go from the starting point to a good image, a coarse approximation of the whished output and a second one from the coarse approximation to the whished output. It may be difficult to avoid the second step but the first one may be shortened using a good starting point.\n",
        "\n",
        "In many situations to compare several methods with various parameters or wavelet bases, it can be useful to compute the PSNR of each output on a benchmark to get a quite fair criterion for the comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9R3r_CJwSle"
      },
      "source": [
        "## Forward Backward for the inpainting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49OeOpBdwSlf"
      },
      "source": [
        "We can start using observations without any noise and apply a mask. To create a 2D mask one can use the following commands : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RszWtBLwSlf"
      },
      "source": [
        "np.random.seed(seed=1)\n",
        "n1,n2=np.shape(im)\n",
        "r=np.random.rand(n1,n2)\n",
        "M=(r<0.8)\n",
        "M = M*1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOjh2wP6wSlf"
      },
      "source": [
        "We can of course change the proportion of pixels that are masked.\n",
        "\n",
        "We can mask the image by the following command :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVCb8xmvwSlf"
      },
      "source": [
        "temp=M*im\n",
        "pn.Row(hv.Image(im).opts(cmap='gray',width=400,height=400),hv.Image(temp).opts(cmap='gray',width=400,height=400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MtoTTMTwSlf"
      },
      "source": [
        "<font color=\"red\">\n",
        "\n",
        "Le masquage a permis d'ajouter des bruits à l'image comme nous le voyons ci-dessus. Selon le seuil de coefficients à conserver, le masquage est plus ou moins fort. Plus la valeur du seuil est élevée, moins le masquage est fort.\n",
        "    </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tgo-_F7wSlf"
      },
      "source": [
        "### Initilisation by median filter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooKYR4pnwSlg"
      },
      "source": [
        "Implement and test a median filter which inpaint roughly the image replacing missing pixels using the median of values of observed pixels on vois*vois squares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02gcwKZfwSlg"
      },
      "source": [
        "def FiltreMedianB(im,masque,vois):\n",
        "    MasIm=masque*im\n",
        "    imrec=np.copy(MasIm)\n",
        "    n1,n2=np.shape(im)\n",
        "    K1=int(np.floor(n1/vois))\n",
        "    K2=int(np.floor(n2/vois))\n",
        "    for cel1 in range(vois):\n",
        "        for cel2 in range(vois):\n",
        "            for i in range(K1):\n",
        "                for j in range(K2):\n",
        "                    if np.abs(imrec[i+cel1*K1,j+cel2*K2])==0:\n",
        "                        imrec[i+cel1*K1,j+cel2*K2]=np.median(imrec[cel1*K1:(cel1+1)*K1,cel2*K2:(cel2+1)*K2])\n",
        "    return(imrec)\n",
        "\n",
        "def FiltreMedian(im,masque,vois):\n",
        "    imrec=np.copy(im)\n",
        "    n1,n2=np.shape(im)\n",
        "    #K1=int(np.floor(n1/vois))\n",
        "    #K2=int(np.floor(n2/vois))\n",
        "    im_mas=masque*im\n",
        "    win=np.zeros(vois**2)\n",
        "    for i in range(np.int(vois/2),n1-np.int(vois/2)):\n",
        "        for j in range(np.int(vois/2),n2-np.int(vois/2)):\n",
        "            if im_mas[i,j]==0:\n",
        "                t=0\n",
        "                for m in range(vois):\n",
        "                    for n in range(vois):\n",
        "                        if im_mas[i+m-np.int(vois/2)][j+n-np.int(vois/2)]!=0:\n",
        "                            win[t]=im_mas[i+m-np.int(vois/2)][j+n-np.int(vois/2)]\n",
        "                            t=t+1\n",
        "                imrec[i,j]=np.median(win[0:t])       \n",
        "    return(imrec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFSpE5gZwSlg"
      },
      "source": [
        "np.random.seed(seed=1)\n",
        "n1,n2=np.shape(im)\n",
        "r=np.random.rand(n1,n2)\n",
        "M=(r<0.5)\n",
        "M = M*1.0\n",
        "irectest=FiltreMedian(im,M,7)\n",
        "pn.Row(hv.Image(im).opts(title=\"Image Initiale\",cmap='gray',width=400,height=400),hv.Image(temp).opts(title=\"Image Bruitée\",cmap='gray',width=400,height=400),hv.Image(irectest).opts(title=\"Image Reconstruite\",cmap='gray',width=400,height=400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMy0mln9wSlg"
      },
      "source": [
        "<font color=\"red\">\n",
        "En comparant les trois images, nous constatons que le fonction $FiltreMedian$ nous a permis de réduire considérablement le bruitage effectué sur l'image initiale.\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FgztMl8wSlg"
      },
      "source": [
        "irectest=FiltreMedian(im,M,1)\n",
        "irectest2=FiltreMedian(im,M,30)\n",
        "pn.Row(pn.Column(hv.Image(im).opts(title=\"Image Initiale\",cmap='gray',width=400,height=400),hv.Image(temp).opts(title=\"Image Bruitée\",cmap='gray',width=400,height=400))\n",
        "       ,pn.Column(hv.Image(irectest).opts(title=\"Image Reconstruite vois=1\",cmap='gray',width=400,height=400),hv.Image(irectest2).opts(title=\"Image Reconstruite vois=60\",cmap='gray',width=400,height=400)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cZjsbomwSlh"
      },
      "source": [
        "<font color=\"red\">\n",
        "    \n",
        "Nous reprenons l'expérience précédente en faisant varier la valeur du paramètre $vois$ de la fonction. Nous constatons que pour de petites valeurs du paramètre, la qualité de l'image reconstruite n'est pas bonne. Elle est même mauvaise. Il en est de même pour de trop grandes valeurs de ce même paramètre où nous obtenons une image assez pixélisée. En effet, si le paramètre $vois$ est trop petit nous n'avons pas assez d'informations lorsque nous calculons la médiane des coefficients voisins. Alors que lorsque nous le prenons trop grand, nous prennons en compte trop d'informations et ainsi l'approximation est assez faussée. \n",
        "    </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbd0mB3dwSlh"
      },
      "source": [
        "### Back to FB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wP0CDmVwSlh"
      },
      "source": [
        "Implement a function computing the gradient of the function $f(x)=\\frac{1}{2}\\Vert M_{1}x-y\\Vert^2$ :\n",
        "\n",
        "<font color=\"red\">\n",
        "    \n",
        "$\\nabla f(x) = M_{1}^{T}(M_{1} x - y)$\n",
        "    \n",
        "Soit  $M_{1}:\\mathbb{R}^{n\\times m}\\to \\mathbb{R}^{n\\times m} $ l'opérateur masque, on définit la matrice M de la taille $n \\times m$:\n",
        "    $M_{1} x=M \\times x$\n",
        "    \n",
        "   Alors l'opérateur $M_{1}$ est une matrice diagonale de taille $(nm)\\times (nm)$ avec les valeurs sous la diagonal sont 1 ou 0\n",
        "    \n",
        " $\\nabla f(x) = M_{1}^{T}(M_{1} x - y)=M_{1}^{T}(M\\times x - y)=M\\times(M\\times x - y)$\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsj1kikNwSlh"
      },
      "source": [
        "def GradientInpainting(x,b,M):\n",
        "    g=M*(M*x-b)\n",
        "    return g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUsHETO1wSlh"
      },
      "source": [
        "For which value of $L$ this gradient is $L-$Lipschitz ?\n",
        "\n",
        "\n",
        "<font color=\"red\">\n",
        "$\\|\\nabla f(a)-\\nabla f(b)\\| = \\|M_{1}^{T}(M_{1} a - y)-M_{1}^{T}(M_{1} b - y)\\|$\n",
        "    \n",
        " $\\|\\nabla f(a)-\\nabla f(b)\\|=\\|M_{1}^{T}(M_{1} (a-b)\\| \\leq \\|M_{1}^{T}(M_{1})\\| \\|a-b\\|$  \n",
        "Le gradient est lipschitzien avec comme coefficient $L=|||M_1|||^2=1$.\n",
        "    </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb5siM7OwSlh"
      },
      "source": [
        "The following functions compute the proximal operator of the function $g(x)=\\lambda \\Vert Tx\\Vert_1$ where $T$ is an orthogonal wavelet transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQjt8GwrwSlh"
      },
      "source": [
        "def SeuillageDouxOndelettes(I,wave,Seuil):\n",
        "    L=pywt.dwt_max_level(len(I),pywt.Wavelet(wave).dec_len)\n",
        "    wavelet_coeffs= pywt.wavedecn(I, wave, mode='per', level=L)\n",
        "    arr, coeff_slices, coeff_shapes = pywt.ravel_coeffs(wavelet_coeffs)\n",
        "    temp=pywt.threshold(arr,Seuil,mode='soft')\n",
        "    test=pywt.unravel_coeffs(temp, coeff_slices, coeff_shapes, output_format='wavedecn')\n",
        "    Irec=pywt.waverecn(test, wave,mode='per')\n",
        "    return Irec\n",
        "def Normel1Ondelettes(I,wave):\n",
        "    L=pywt.dwt_max_level(len(I),pywt.Wavelet(wave).dec_len)\n",
        "    wavelet_coeffs= pywt.wavedecn(I, wave, mode='per', level=L)\n",
        "    arr, coeff_slices, coeff_shapes = pywt.ravel_coeffs(wavelet_coeffs)\n",
        "    norml1=sum(np.abs(arr))\n",
        "    return norml1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCGnhUT3wSli"
      },
      "source": [
        "wave='haar'\n",
        "imrec=SeuillageDouxOndelettes(im,wave,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0AIQiNYwSli"
      },
      "source": [
        "Implement and test an inpainting program using the FB algorithm minimizing the function \\eqref{Eq1}.\n",
        "Don't forget to clip the data at the end of the program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj_xqi8qwSli"
      },
      "source": [
        "def ForwardBackwardInpaintingv2(y,M,step,lam,Niter,wave):\n",
        "    seuil=lam*step\n",
        "    #z=0*y\n",
        "    z=FiltreMedian(y,M,7)\n",
        "    F=np.zeros(Niter)\n",
        "    for iter in range(Niter):\n",
        "        z=SeuillageDouxOndelettes(z-step*GradientInpainting(z,y,M),wave,seuil)\n",
        "        F[iter]=0.5*npl.norm(M*z-y)**2+lam*Normel1Ondelettes(z,wave)\n",
        "    return np.clip(z,0,255),F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hx77VTNwSli"
      },
      "source": [
        "lam=5\n",
        "Niter=200\n",
        "wave='db2'\n",
        "np.random.seed(seed=1)\n",
        "n1,n2=np.shape(im)\n",
        "r=np.random.rand(n1,n2)\n",
        "M=(r<0.5)\n",
        "M = M*1.0\n",
        "B=np.random.randn(n1,n2)\n",
        "y=M*im+5*B\n",
        "y=np.clip(y,0,255)\n",
        "step=0.9\n",
        "Z,CF=ForwardBackwardInpaintingv2(y,M,step,lam,Niter,wave)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBo0ynI9wSli"
      },
      "source": [
        "pn.Row(hv.Image(im).opts(cmap='gray',width=400,height=400),hv.Image(Z).opts(cmap='gray',width=400,height=400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oB0U8lXwSli"
      },
      "source": [
        "plt.plot(CF)\n",
        "print(CF[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Py8sODNwSli"
      },
      "source": [
        "<font color=\"red\">\n",
        "Nous constatons que l'algorithme est bien un algorithme de descente. En effet, la fonction coût décroit au cours du temps. Lorsqu'on observe l'image obtenue, nous pouvons constater que nous obtenons une image d'assez bonne qualité même si la valeur minimale de la fonction coût reste élevé. Afin de mieux visualiser les effets des paramètres sur l'image reconstruite, nous allons construire un dashboard et visualiser les effets.\n",
        "    </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWN17M8pwSlj"
      },
      "source": [
        "wavelist = ['haar','db2','db3','db4','coif1','coif2','coif3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8-tt_hSwSlj"
      },
      "source": [
        "Implement a dashboard allowing a numerical exploration of the previous function. We could observe that small small values of $\\lambda$ produce better output. Theoreticaly, if the data are noiseless, it would be better to take $\\lambda$ as small as possible. You can see it if you display the PSNR of the output when you compute a number of iterations that is large enough.\n",
        "\n",
        "The drawback of such a choice is that it slows down extremely the algorithm. If data are noisy the optimal value of $\\lambda$ is proportional to the standart deviation of the noise.\n",
        "\n",
        "Hence, you may prefer a small value of $\\lambda$ but nottoo small to ensure the convergence of the algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GUz7ku3wSlj"
      },
      "source": [
        "class FBInpaint(param.Parameterized):\n",
        "    wave = param.ObjectSelector(default=\"db2\",objects=wavelist)\n",
        "    image = param.ObjectSelector(default=\"Canaletto\",objects=imagesRef.keys())\n",
        "    Niter = param.Integer(10,bounds=(0,300))\n",
        "    lam = param.Number(5,bounds=(1,30))\n",
        "    step = param.Number(1,bounds=(0.005,4))\n",
        "    masquage = param.Number(0.5,bounds=(0.1,1))\n",
        "    sigma=param.Number(5,bounds=(0,10))\n",
        "    def view(self):\n",
        "        ima=imagesRef[self.image]\n",
        "        n1,n2=np.shape(ima)\n",
        "        r=np.random.rand(n1,n2)\n",
        "        M=(r<self.masquage)\n",
        "        M = M*1.0\n",
        "        B=np.random.randn(n1,n2)\n",
        "        y=M*ima+self.sigma*B\n",
        "        Isol,CF=ForwardBackwardInpaintingv2(y,M,self.step,self.lam,self.Niter,self.wave)\n",
        "        \n",
        "        #calcul du PSNR\n",
        "        p1 = PSNR(y,im)\n",
        "        p2 = PSNR(Isol,im)\n",
        "        \n",
        "        #affichage\n",
        "        strp1=\"%2.2f\" % p1\n",
        "        strp2=\"%2.2f\" % p2\n",
        "        te1='PSNR image masque = '\n",
        "        te2='PSNR image inpaint = '\n",
        "        TN1=hv.Text(0.5,0.3,te1+strp1).opts(xaxis=None,yaxis=None,toolbar=None)\n",
        "        TN2=hv.Text(0.5,0.7,te2+strp2).opts(xaxis=None,yaxis=None)\n",
        "        \n",
        "        return pn.Row(pn.Column(hv.Image(y).opts(title=\"Image Bruitée\",cmap='gray',width=400,height=400),hv.Image(Isol).opts(title=\"Image reconstruite\",cmap='gray',width=400,height=400),plt.plot(CF)),TN1*TN2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtUH4iBZwSlj"
      },
      "source": [
        "fbinpaint= FBInpaint()\n",
        "pn.Row(fbinpaint.param,fbinpaint.view)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGcokaGEwSlj"
      },
      "source": [
        "<font color = red>\n",
        "    \n",
        "Tout d'abord, l'image reconstruit est acceptable quand la nombre d'itération est 50. Toutefois, nous pouvons augementer le nombre d'itérations pour améliorer le résultat. Quand le nombre d'itérations est environ 200, la fonction coût se stabilise et notre algorithme converge. \n",
        "    \n",
        "Le paramètre $\\lambda$ n'apporte pas de gros changements. En effet, la valeur PSNR augmente jusqu'à une valeur optimale autour de $10$ puis décroît légèrement même si ces valeurs restent très proches. \n",
        "    \n",
        "Nous constatons également que plus la valeur du pas est grande, moins l'image reconstruite est de bonne qualité. Lorsque cette valeur dépasse $2$, l'image de très mauvaise qualité. En effet, l'algorithme FB converge pour de valeurs du pas inférieur à $\\frac{2}{L}$ avec $L=|||M_1|||^2$. Vu comment la matrice $M_1$ est construite, nous devons avoir $L=1$. Ainsi l'algorithme converge pour des valeurs de  $step$ inférieures à $2$.\n",
        "    \n",
        "Nous trouvons aussi que  la base d'ondelette utilisée a peu d'impact sur l'image reconstruite. \n",
        "Nous voyons aussi que quand la valeur de masque est plus grande, l'opérateur masque retient plus de pixels donc l'image bruitée est plus nette. D'où, l'image débruitée est meilleure.\n",
        "    \n",
        "</font>    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv2pcv8BwSlj"
      },
      "source": [
        "# Accélération de Nesterov, FISTA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIq-jqKgwSlj"
      },
      "source": [
        "Yurii Nesterov has proposed in the 80's several methods to accelerate the explicit Gradient Descent. We will focus on one of these, the one that is described in the lesson. If you have a look to research paper be aware that the 2 words \"Nesterov accelerations\" may have several meanings... using interpolation or extrapolation, be specific to strongly convex functions or not. it may apply to the explicit Gradient Descent (GD) or to the Forward Backward algorithm. Moreover we will not use the original parameter of Nesterov or FISTA but the ones we proposed in 2014 with Antonin Chambolle to ensure the convergence of iterates and speed up the convergence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCBHoRlGwSlk"
      },
      "source": [
        "the acceleration of the GD proposed but Nesterov in 19!4 and adapted to FB under the name of FISTA by Beck and Teboulle in 2009 is easy the apply and without any additional computational cost.\n",
        "\n",
        "The idea is to apply the FB (or the GD for a single differntiable function) to a shifted point of $x_n$ with a step $h<\\frac{1}{L}$. \n",
        "Be careful this bound is more restrictive than the one for the classical GD :\n",
        "$$x_{n+1}=T(x_n+\\alpha_n(x_n-x_{n-1}))$$ \n",
        "where the sequence $\\alpha_n$ is chosen in a suitable way and $T$ is the FB operator. We say that FISTA is an inertial method because it uses an inertial term as a memory of the last descent direction.\n",
        "\n",
        "Let's give some key points of this method :\n",
        "1) The original choice of Nesterov for the sequence $\\alpha_n$ is the following :\n",
        "\\begin{equation}\n",
        "\\alpha_n=\\frac{t_n-1}{t_{n+1}}\\text{ avec }t_1=1\\text{ et }t_{n+1}=\\frac{1+\\sqrt{1+t_n^2}}{2}\n",
        "\\end{equation}\n",
        "2) For this choice we have \n",
        "$$F(x_n)-F(x^*)\\leqslant \\frac{2\\Vert x_0-x^*\\Vert^2}{hn^2}$$\n",
        "3) We can take the more simple choice $\\alpha_n=\\frac{n-1}{n+a-1}$ with $a>3$ (and that's what you will implement) and in this cas we have \n",
        "$$F(x_n)-F(x^*)\\leqslant \\frac{(a-1)^2\\Vert x_0-x^*\\Vert^2}{2h(n+a)^2}$$ and moreover \n",
        "$F(x_n)-F(x^*)=o\\left(\\frac{1}{n^2}\\right)$ and the sequence $(x_n)_{n\\geqslant 1}$ converges. \n",
        "It can be noticed, there are no inertia in the first step ($\\alpha_1=0$) and thus $x_1=T(x_0)$. \n",
        "The inertia appears for the computation of $x_2$. The original choice of Nesterov is very close to the choice $\\alpha=3$.\n",
        "\n",
        "4) In the case of a composite fucntion ($F$ is a sum of a diffrentiable fucntion $f$ and a possibly non smooth fucntion $g$), this inertial algorithm is called FISTA (Fast Iterative Soft Shrinkage Thresholding Algorithm) because when $g$ is the $\\ell_1$ norm, the proximal operator is a soft thresholding but the name FISTA is the name of this algorithm even if $g$ is not an $\\ell_1$ norm.\n",
        "\n",
        "5) Unlike FB, the sequence $(F(x_n)-F(x^*))_{n\\geqslant 1}$ given by FISTA is not necessarily non increasing. The previous bounds on FB or FISTA are only ... bounds... but does not describe the real decay rate.\n",
        "In practical cases you should see that FISTA is often faster than FB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrU0GvqwSlk"
      },
      "source": [
        "Implement an inpaiting algorithm minimizing the fucntion \\eqref{Eq1} using FISTA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KGJRpH0wSlk"
      },
      "source": [
        "def FISTAInpainting(y,M,step,lam,Niter,wave,alpha):\n",
        "    seuil=lam*step\n",
        "    #z=0*y\n",
        "    z=FiltreMedian(y,M,8)\n",
        "    F=np.zeros(Niter)\n",
        "    z_old=z\n",
        "    for k in np.arange(0,Niter):\n",
        "        a = (k-1)/(k+alpha-1)\n",
        "        w = z + a*(z-z_old)\n",
        "        grad = GradientInpainting(w,y,M)\n",
        "        z_old = z\n",
        "        z = SeuillageDouxOndelettes(w - step*grad,wave,seuil)\n",
        "        F[k] =  lam*Normel1Ondelettes(z,wave)+1/2*np.linalg.norm(M*z-y)**2\n",
        "      \n",
        "        \n",
        "    return np.clip(z,0,255),F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBaeiPMnwSlk"
      },
      "source": [
        "lam=10\n",
        "Niter=30\n",
        "wave='db2'\n",
        "np.random.seed(seed=1)\n",
        "n1,n2=np.shape(im)\n",
        "r=np.random.rand(n1,n2)\n",
        "M=(r<0.5)\n",
        "M = M*1.0\n",
        "B=np.random.randn(n1,n2)\n",
        "y=M*im+5*B\n",
        "y=np.clip(y,0,255)\n",
        "step=0.9\n",
        "alpha=3.1\n",
        "Z,CF=FISTAInpainting(y,M,step,lam,Niter,wave,alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnCVsYXCwSlk"
      },
      "source": [
        "pn.Row(hv.Image(im).opts(cmap='gray',width=400,height=400),hv.Image(Z).opts(cmap='gray',width=400,height=400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7juFP9mfwSlk"
      },
      "source": [
        "plt.plot(CF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rImo0yhRwSlk"
      },
      "source": [
        "class FISTAInpaint(param.Parameterized):\n",
        "    wave = param.ObjectSelector(default=\"db2\",objects=wavelist)\n",
        "    image = param.ObjectSelector(default=\"Canaletto\",objects=imagesRef.keys())\n",
        "    Niter = param.Integer(20,bounds=(0,300))\n",
        "    lam = param.Number(10,bounds=(1,30))\n",
        "    step = param.Number(0.9,bounds=(0.05,4))\n",
        "    masquage = param.Number(0.5,bounds=(0.1,1))\n",
        "    alpha = param.Number(3.1,bounds=(3,10))\n",
        "    sigma=param.Number(5,bounds=(0,20))\n",
        "    def view(self):\n",
        "        ima=imagesRef[self.image]\n",
        "        n1,n2=np.shape(ima)\n",
        "        r=np.random.rand(n1,n2)\n",
        "        r2=np.random.rand(n1,n2)\n",
        "        M=(r<self.masquage)\n",
        "        M = M*1.0\n",
        "        B=np.random.randn(n1,n2)\n",
        "        \n",
        "        y=M*ima+self.sigma*B\n",
        "        y=np.clip(y,0,255)\n",
        "        Isol,CF=FISTAInpainting(y,M,self.step,self.lam,self.Niter,self.wave,self.alpha)\n",
        "        \n",
        "       #calcul du PSNR\n",
        "        p1 = PSNR(y,im)\n",
        "        p2 = PSNR(Isol,im)\n",
        "        \n",
        "        #affichage\n",
        "        strp1=\"%2.2f\" % p1\n",
        "        strp2=\"%2.2f\" % p2\n",
        "        te1='PSNR image masque = '\n",
        "        te2='PSNR image inpaint = '\n",
        "        TN1=hv.Text(0.5,0.3,te1+strp1).opts(xaxis=None,yaxis=None,toolbar=None)\n",
        "        TN2=hv.Text(0.5,0.7,te2+strp2).opts(xaxis=None,yaxis=None)\n",
        "        \n",
        "        return pn.Row(pn.Column(hv.Image(Isol).opts(cmap='gray',width=400,height=400),plt.plot(CF)),TN1*TN2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "CZMmAp9ewSll"
      },
      "source": [
        "fistainpaint= FISTAInpaint()\n",
        "pn.Row(fistainpaint.param,fistainpaint.view)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqjKXGhZwSll"
      },
      "source": [
        "<font color = red>\n",
        "   \n",
        "Nous voyons que l'algorithme converge très vite, seulement en plus 15 iterations. Les paramètres comme la $base d'ondelette$, $Lam$, $Step$, $Masquage$ conservent la même fonction comme l'algorithme Forward-Backward. Pour le paramètre $Alpha$, on trouve que la valeur optimale est autour de 3.\n",
        "    \n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX9ngSqnwSll"
      },
      "source": [
        "## To go further (optional )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osVREqFqwSll"
      },
      "source": [
        "As it has been done for denoising, one can use translated wavelets to improve the quality of the inpaiting. \n",
        "if you want to use this trick, you don't have to compute the inpainting from the beginning for each wavelet basis but use the previous outputs as smart starting points for FISTA or FB. This variation of the algorithm allows to smooth some artefacts due to the shape of the chosen wavelet.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiBonP-kwSll"
      },
      "source": [
        "## Denoising using Total Variation norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bA92fM2wSll"
      },
      "source": [
        "If we know that the image we want to denoise has a small Total Variation norm ($\\ell_1$ norm of the gradient), one can estimate the image minimizing a fucntion. be careful, the gradient is the one of the image not of a funtion applyed to an image. the gradient of an image is a vector field with two components, a vertical and a horizontal one which are computed using finite differences. I made a choice in the following programs for the discretization of the gradient and of the associated adjoint operator the negative divergence. If you ant to use another discretization of the gradient, you will have to modify the computation of the negative divergence. \n",
        "\n",
        "If we denote $y=x^0+b$ the observations where $x^0$ is the image to estimate and $b$ an additive noise, this fucntion will be\n",
        "\\begin{equation}\\label{Primal}\\tag{Primal}\n",
        "\\frac{1}{2}\\Vert x-y\\Vert^2+\\lambda \\Vert \\nabla x\\Vert_1\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rmz7YnSwSll"
      },
      "source": [
        "Minimize such a fucntion is equivalent to compute the proximal operator of the function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeIrlp_lwSll"
      },
      "source": [
        "Minimiser une telle fonctionnelle revient à caluler l'opérateur proximal de la fonction $g(x)=\\lambda \\Vert \\nabla x\\Vert_1$ but it does not exist any formul for such an operator. It also means that we can't directly apply the FB using the quadratic term as a differentiable function.\n",
        "\n",
        "However, it exists a way to use the FB algorithm to solve this problem : Duality.\n",
        "It is not possible to give many details about this approach and I ask you to read your notes about duality and conjugate fucntions. The dualisation which uses the notion of Fenchel-Rockafellar conjugate associates to the previous optimization problem the follwiong one  \n",
        "\\begin{equation}\\label{Dual}\\tag{Dual}\n",
        "\\min_{p}\\frac{1}{2}\\Vert div(p)+y\\Vert^2+\\iota_{\\mathcal{B}_{\\infty,\\lambda}}(p)\n",
        "\\end{equation}\n",
        "where $p$ is a gradient field and $\\iota_{\\mathcal{B}_{\\infty,\\lambda}}$ is the function equal to 0 on the ball \n",
        "$\\ell_{\\infty}$ of radius $\\lambda$ and $+\\infty$ ouside and $div$ is the divergence operator. We have a divergence here because the \n",
        "adjoint of the gradient operator is the negative divergence ans a $\\ell_{\\infty}$ ball because it is the conjugate function of the $\\ell_1$ norm.\n",
        "\n",
        "If $p^*$ is a solution of the previous dual problem then $x^*=y+div(p^*)$ is the solution of the primal original problem. The advantage of the second problem is that it can be solved using FB or FISTA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj8mAAKxwSll"
      },
      "source": [
        "The following functions compute the discrete gradient and the associated discrete negative divergence.\n",
        "As previously said, it is a choice of dsicretization, not the only one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aruSWwdcwSlm"
      },
      "source": [
        "def GradientHor(x):\n",
        "    y=x-np.roll(x,1,axis=1)\n",
        "    y[:,0]=0\n",
        "    return y\n",
        "def GradientVer(x):\n",
        "    y=x-np.roll(x,1,axis=0)\n",
        "    y[0,:]=0\n",
        "    return y\n",
        "def DivHor(x):\n",
        "    N=len(x[0])\n",
        "    y=x-np.roll(x,-1,axis=1)\n",
        "    y[:,0]=-x[:,1]\n",
        "    y[:,N-1]=x[:,N-1]\n",
        "    return y\n",
        "def DivVer(x):\n",
        "    N=len(x)\n",
        "    y=x-np.roll(x,-1,axis=0)\n",
        "    y[0,:]=-x[1,:]\n",
        "    y[N-1,:]=x[N-1,:]\n",
        "    return y\n",
        "def Gradient(x):\n",
        "    y=[]\n",
        "    y.append(GradientHor(x))\n",
        "    y.append(GradientVer(x))\n",
        "    return y\n",
        "def Div(y):\n",
        "    x=DivHor(y[0])+DivVer(y[1])\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73pPzpWvwSlm"
      },
      "source": [
        "def ProjGradBouleInf(g,l):\n",
        "    gh=g[0]\n",
        "    gv=g[1]\n",
        "    temp=g\n",
        "    p0=gh-(gh-l)*(gh>l)-(gh+l)*(gh<-l)\n",
        "    p1=gv-(gv-l)*(gv>l)-(gv+l)*(gv<-l)\n",
        "    temp[0]=p0\n",
        "    temp[1]=p1\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuFyX2niwSlm"
      },
      "source": [
        "Implement a Python function solving the dual problem using the FB algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZbC_InDwSlm"
      },
      "source": [
        "def FBDenoisingTV(y,l,step,Niter):\n",
        "    x=0*y\n",
        "    p=Gradient(x)\n",
        "    for k in range(0,Niter):\n",
        "        \n",
        "    return np.clip(x,0,255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4EtdYQzwSlm"
      },
      "source": [
        "Test the program on a example on a noisy image with additional gaussian noise. Why can you observe an cartoon effect on the output image ? Change the values of parameters to check the effects of each of them on the output.\n",
        "The stemp must be set above $1/4$ to satisfy the condition $h<\\frac{2}{L}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QclC2kPHwSlm"
      },
      "source": [
        "Create the associated dashboard and test the algorithm on different images and parameters. The output is three images (original, noisy and solution) and 2 PSNR (of the noisy image and of the output). You should observe a cartoon effect for large values of $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MroF65HNwSlm"
      },
      "source": [
        "class DenoisingTVFB(param.Parameterized):\n",
        "    Niter = param.Integer(100,bounds=(0,500))\n",
        "    image = param.ObjectSelector(default=\"Canaletto\",objects=imagesRef.keys())\n",
        "    lam = param.Number(9,bounds=(1,50))\n",
        "    step = param.Number(0.1,bounds=(0.1,2))\n",
        "    Sigma = param.Number(17,bounds=(1,100))\n",
        "    #alpha = param.Number(3,bounds=(1,10))\n",
        "    def view(self):\n",
        "        \n",
        "        return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFcL-4tUwSlm"
      },
      "source": [
        "denoisingTVFB=DenoisingTVFB()\n",
        "pn.Row(denoisingTVFB.param,denoisingTVFB.view)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQJpjCvswSln"
      },
      "source": [
        "# And Next ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo2A9NDDwSln"
      },
      "source": [
        "1) One can speed up the denoising intitiazing by a wavelet thresholding... one may compare the two approaches with or without translations using PSNR.\n",
        "2) Apply FISTA.\n",
        "3) Compute an Experiments Plan to compare the different choices of parameters. YHou must be careful here because the computational time is quite large for each experiements. You must have an idea of the total time of your Experiment Plan before starting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU8Rt1gQwSln"
      },
      "source": [
        "## Going further (optional)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHtzIpGnwSln"
      },
      "source": [
        "### Several defintions of the Total Variation (TV) Norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDL0QH6RwSln"
      },
      "source": [
        "It exists several definitions of the TV norm . The one we are using here is the anisotropic TV norm, it also exists an isotropic one :\n",
        "\n",
        "https://en.wikipedia.org/wiki/Total_variation_denoising\n",
        "\n",
        "The use of the isotropic TV norm leads to artefacts that are more homogenious in all directions. \n",
        "The difference between the two approaches are particularly visible on the corners and the sides of objects.\n",
        "\n",
        "To deal with an isotropic norm, you have to change the norme in \\eqref{Primal}, and thus the definition of the \n",
        "$\\ell_\\infty$ ball in the dual problem is a bit different.\n",
        "\n",
        "You must be aware that this difference exists and be clear in your mind about the norms you are using. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYPoVWijwSln"
      },
      "source": [
        "### Denoising of colored images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvoac7ejwSln"
      },
      "source": [
        "It is possible de denoise colored images, applying the previous algorithms on each chromatic chanel, but it is also possible to work directly on the colored image and define a global 3D TV norm on the three chanels, summing over the whole image, the norm of a gradient with ... 6 components. The goal of this approach is to avoid incoherence between the 3 chromatic chanels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAkQXg1KwSln"
      },
      "source": [
        "## Take away message."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyOwDzdkwSln"
      },
      "source": [
        "If you implement a FB algorithm, nesterov accelerations are often efficient and it is always useful to test and even to test several choices of $a$. beginning with $a=3$. One can notive that, at least theoretically, the sequence   \n",
        "generated $(F(x_n)-F(x^*))_{n\\geqslant 1}$ generated by FB as a geometrical decay when $F$ is strongly convex, which is not the case for the studied Nesterov scheme. But it exists some accelerations that are dedicated to this specific situation. In practical cases, for a reasonnable number of iterations, these Nesterov accelerations are really competitive."
      ]
    }
  ]
}